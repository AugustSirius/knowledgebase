# 数据分析与统计

> **类别**: 科研能力建设  
> **生成时间**: 2025-10-30 16:10:36  
> **能力描述**: 统计方法选择、数据清洗、可视化、可重复性、开放科学

---

# 数据分析与统计：实战掌握指南

## 1. 能力本质与重要性

### 核心本质
数据分析与统计的核心是**将不确定性转化为可操作的洞察**。这不是数学游戏，而是一种决策工具——让你在噪音中找到信号，在混沌中发现模式，在不完美数据中做出最优判断。

对生物AI研究者而言，这是**将生物学直觉转化为可验证假设**的桥梁，是**让投资人相信你的技术可行性**的语言，是**在顶级期刊发表论文**的必备武器。

### 对你多重目标的关键价值

**科研维度：**
- 虚拟细胞模型需要整合多组学数据（基因组、转录组、蛋白质组），统计方法决定模型准确度
- 精准医疗的核心是从患者数据中识别亚型和预测响应——这是纯粹的统计问题
- 顶级期刊（Nature/Science/Cell）的审稿人会严格审查统计方法，错误选择会直接导致拒稿

**创业维度：**
- 投资人要看数据：用户增长曲线、A/B测试结果、市场规模估算——所有这些都需要统计支撑
- FDA/NMPA审批需要严格的统计设计（临床试验的样本量计算、多重检验校正）
- 产品迭代依赖数据驱动决策，错误的统计会让你优化错误的指标

**影响力维度：**
- 科研圈：可重复性危机让统计严谨性成为信誉标志
- 企业家圈：数据驱动决策是现代管理的通用语言
- 政治圈：政策制定者需要基于证据的建议，统计是证据的语言

### 掌握vs不掌握的差异

**掌握者：**
- 能在3小时内从原始数据得出可靠结论，写出令人信服的报告
- 避免90%的研究陷阱（p-hacking、过拟合、辛普森悖论）
- 融资时能用数据讲故事，让投资人产生"这个团队专业"的信任感
- 论文被拒后能快速识别统计问题并修正，而不是盲目重投

**未掌握者：**
- 花费数月收集数据，却因统计方法错误导致结果不可信
- 被审稿人的统计质疑击垮，论文反复修改仍无法发表
- 创业时凭直觉决策，错失数据中的关键信号
- 在精英圈层讨论中暴露统计盲点，损害专业形象

**量化差异：**
- 论文接受率：统计严谨的研究接受率高40-60%
- 融资成功率：数据驱动的pitch成功率是纯愿景型的3倍
- 决策质量：A/B测试可提升产品指标20-50%
- 时间成本：正确的统计设计可节省50-70%的后期返工时间

---

## 2. 实操框架与方法论

### 核心框架：QVIR循环（Question-Visualize-Infer-Reproduce）

这是我为生物医学研究者设计的实战框架，每个环节都有具体操作清单。

### **阶段1：问题定义（Question）- 第1-2周**

**目标：**将模糊想法转化为可检验的统计假设

**具体步骤：**
1. **写下PICO框架**（医学研究的黄金标准）
   - Population（人群）：研究对象是谁？
   - Intervention（干预）：你在测试什么？
   - Comparison（对照）：与什么比较？
   - Outcome（结果）：测量什么指标？

2. **确定数据类型和分析层级**
   ```
   数据类型决策树：
   - 连续变量（如基因表达量）→ t检验/ANOVA/回归
   - 分类变量（如疾病类型）→ 卡方检验/逻辑回归
   - 时间序列（如细胞生长曲线）→ 时间序列分析/生存分析
   - 高维数据（如组学数据）→ 降维/聚类/机器学习
   ```

3. **计算样本量**（避免最常见的错误）
   - 使用G*Power软件（免费）
   - 设定：α=0.05, β=0.2（80%检验效能）
   - 预估效应量（查阅同类研究或进行预实验）
   - **关键：**样本量不足是90%失败研究的根源

**工具：**
- G*Power（样本量计算）
- OSF预注册平台（记录假设，防止p-hacking）

**时间投入：**每个项目2-4小时

---

### **阶段2：数据清洗（第3-6周）**

**目标：**将杂乱数据转化为分析就绪的格式

**黄金流程（Tidy Data原则）：**

```python
# 标准数据清洗Pipeline（Python示例）
import pandas as pd
import numpy as np

# 1. 加载并初步检查
df = pd.read_csv('raw_data.csv')
print(df.info())  # 查看数据类型和缺失值
print(df.describe())  # 查看分布

# 2. 处理缺失值（关键决策点）
# 策略A：删除（缺失<5%）
df_clean = df.dropna(subset=['critical_variable'])

# 策略B：插补（缺失5-20%）
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
df[['var1', 'var2']] = imputer.fit_transform(df[['var1', 'var2']])

# 策略C：建模（缺失>20%，使用MICE）
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
imputer = IterativeImputer(random_state=42)

# 3. 异常值检测
from scipy import stats
z_scores = np.abs(stats.zscore(df['expression']))
df_clean = df[(z_scores < 3)]  # 移除3倍标准差外的点

# 4. 数据转换
df['log_expression'] = np.log2(df['expression'] + 1)  # 基因表达常用
df['normalized'] = (df['value'] - df['value'].mean()) / df['value'].std()

# 5. 验证清洗结果
assert df_clean.isnull().sum().sum() == 0, "仍有缺失值"
assert len(df_clean) > 0.8 * len(df), "删除了超过20%数据，重新评估策略"
```

**关键检查清单：**
- [ ] 每个变量的分布是否合理？（直方图检查）
- [ ] 是否有编码错误？（如性别出现3个类别）
- [ ] 时间戳是否连续？
- [ ] 单位是否一致？（常见错误：混用mg和μg）
- [ ] 是否有重复记录？

**工具：**
- Python: pandas, numpy, scikit-learn
- R: tidyverse, naniar（缺失值可视化）
- OpenRefine（非编程用户的清洗神器）

**时间投入：**
- 小型数据集（<10K行）：4-8小时
- 中型数据集（10K-1M行）：2-5天
- 大型组学数据：1-2周

---

### **阶段3：可视化探索（第7-10周）**

**目标：**在统计检验前发现数据的故事

**分层可视化策略：**

**Level 1：单变量分布（第一周）**
```python
import seaborn as sns
import matplotlib.pyplot as plt

# 连续变量
sns.histplot(data=df, x='gene_expression', kde=True)
plt.title('Distribution of Gene Expression')

# 箱线图检测异常值
sns.boxplot(data=df, y='expression')

# 分类变量
sns.countplot(data=df, x='disease_type')
```

**Level 2：双变量关系（第二周）**
```python
# 散点图 + 回归线
sns.regplot(data=df, x='age', y='biomarker')

# 分组比较
sns.violinplot(data=df, x='treatment', y='outcome')

# 相关性热图
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
```

**Level 3：多变量模式（第三周）**
```python
# PCA降维可视化（组学数据必备）
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df_numeric)

plt.scatter(principal_components[:, 0], 
            principal_components[:, 1],
            c=df['disease_type'])
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')

# t-SNE（非线性降维）
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
tsne_results = tsne.fit_transform(df_numeric)
```

**Level 4：出版级图表（第四周）**
```python
# 使用ggplot风格
plt.style.use('seaborn-v0_8-paper')

# 组合图（Figure with subplots）
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 添加统计注释
from statannotations.Annotator import Annotator
annotator = Annotator(ax, pairs=[("Control", "Treatment")],
                     data=df, x='group', y='value')
annotator.configure(test='t-test_ind', text_format='star')
annotator.apply_and_annotate()

# 保存高分辨率图片
plt.savefig('figure1.pdf', dpi=300, bbox_inches='tight')
```

**可视化决策树：**
```
目标是什么？
├─ 展示分布 → 直方图/小提琴图
├─ 比较组间差异 → 箱线图/条形图
├─ 展示相关性 → 散点图/热图
├─ 展示趋势 → 折线图
├─ 展示组成 → 饼图/堆叠条形图
└─ 探索高维数据 → PCA/t-SNE
```

**工具：**
- Python: matplotlib, seaborn, plotly（交互式）
- R: ggplot2（最强大的可视化库）
- Tableau/Power BI（商业报告）
- BioRender（生物学示意图）

**时间投入：**每个项目10-20小时（含迭代）

---

### **阶段4：统计推断（第11-16周）**

**目标：**从样本推断总体，量化不确定性

**统计方法选择流程图：**

```
研究问题
│
├─ 比较组间差异
│  ├─ 2组 + 连续变量 → t检验（正态）/ Mann-Whitney U（非正态）
│  ├─ 3+组 + 连续变量 → ANOVA（正态）/ Kruskal-Wallis（非正态）
│  ├─ 2组 + 分类变量 → 卡方检验 / Fisher精确检验
│  └─ 配对数据 → 配对t检验 / Wilcoxon符号秩检验
│
├─ 探索关联
│  ├─ 连续-连续 → Pearson相关（线性）/ Spearman相关（单调）
│  ├─ 连续-分类 → 逻辑回归 / 线性判别分析
│  └─ 分类-分类 → 卡方检验 / 对数线性模型
│
├─ 预测建模
│  ├─ 连续结果 → 线性回归 / 随机森林回归
│  ├─ 分类结果 → 逻辑回归 / SVM / 神经网络
│  └─ 时间-事件 → Cox回归 / Kaplan-Meier
│
└─ 高维数据
   ├─ 聚类 → K-means / 层次聚类 / DBSCAN
   ├─ 降维 → PCA / t-SNE / UMAP
   └─ 特征选择 → LASSO / 随机森林重要性
```

**实战代码模板：**

**案例1：比较两组基因表达**
```python
from scipy import stats
import numpy as np

# 数据准备
control = df[df['group'] == 'control']['expression']
treatment = df[df['group'] == 'treatment']['expression']

# 1. 检验正态性
_, p_norm_control = stats.shapiro(control)
_, p_norm_treatment = stats.shapiro(treatment)

if p_norm_control > 0.05 and p_norm_treatment > 0.05:
    # 正态分布 → t检验
    t_stat, p_value = stats.ttest_ind(control, treatment)
    test_used = "Independent t-test"
else:
    # 非正态 → Mann-Whitney U
    u_stat, p_value = stats.mannwhitneyu(control, treatment)
    test_used = "Mann-Whitney U test"

# 2. 计算效应量（Cohen's d）
pooled_std = np.sqrt((control.std()**2 + treatment.std()**2) / 2)
cohens_d = (treatment.mean() - control.mean()) / pooled_std

# 3. 报告结果
print(f"{test_used}: p = {p_value:.4f}")
print(f"Effect size (Cohen's d): {cohens_d:.2f}")
print(f"Interpretation: {'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'}")
```

**案例2：多重比较校正（组学数据必备）**
```python
from statsmodels.stats.multitest import multipletests

# 假设你有1000个基因的p值
p_values = [...]  # 1000个p值

# 方法1：Bonferroni（最保守）
reject_bonf, pvals_bonf, _, _ = multipletests(p_values, method='bonferroni')

# 方法2：FDR（Benjamini-Hochberg，推荐）
reject_fdr, pvals_fdr, _, _ = multipletests(p_values, method='fdr_bh')

# 结果对比
print(f"Significant genes (Bonferroni): {reject_bonf.sum()}")
print(f"Significant genes (FDR): {reject_fdr.sum()}")

# 创建结果表
results_df = pd.DataFrame({
    'gene': gene_names,
    'p_value': p_values,
    'p_adj_bonf': pvals_bonf,
    'p_adj_fdr': pvals_fdr,
    'significant_fdr': reject_fdr
})
```

**案例3：生存分析（临床研究核心）**
```python
from lifelines import KaplanMeierFitter, CoxPHFitter
from lifelines.statistics import logrank_test

# Kaplan-Meier曲线
kmf = KaplanMeierFitter()

for group in df['treatment'].unique():
    mask = df['treatment'] == group
    kmf.fit(df[mask]['time'], df[mask]['event'], label=group)
    kmf.plot()

# Log-rank检验
results = logrank_test(df[df['treatment']=='A']['time'],
                       df[df['treatment']=='B']['time'],
                       df[df['treatment']=='A']['event'],
                       df[df['treatment']=='B']['event'])
print(f"Log-rank test p-value: {results.p_value:.4f}")

# Cox回归（调整协变量）
cph = CoxPHFitter()
cph.fit(df, duration_col='time', event_col='event')
cph.print_summary()
```

**关键原则：**
1. **永远先检验假设**（正态性、方差齐性、独立性）
2. **报告效应量，不只是p值**（p<0.05不代表重要）
3. **多重比较必须校正**（否则假阳性率爆炸）
4. **使用置信区间**（比单点估计更有信息量）

**工具：**
- Python: scipy.stats, statsmodels, pingouin
- R: stats, lme4（混合效应模型）, survival
- SPSS/SAS（如果合作者要求）

**时间投入：**
- 基础检验：2-4小时
- 复杂模型（混合效应、贝叶斯）：1-2周
- 临床试验设计：2-4周

---

### **阶段5：可重复性与开放科学（第17-20周）**

**目标：**让你的分析可被任何人重现

**黄金标准工作流：**

**1. 版本控制（Git + GitHub）**
```bash
# 初始化项目
git init
git add .
git commit -m "Initial commit: raw data and analysis plan"

# 创建分支进行实验性分析
git checkout -b exploratory-analysis

# 合并到主分支
git checkout main
git merge exploratory-analysis

# 推送到GitHub（公开或私有）
git remote add origin https://github.com/yourusername/project.git
git push -u origin main
```

**2. 环境管理（避免"在我电脑上能跑"问题）**
```bash
# Python: 使用conda
conda create -n myproject python=3.9
conda activate myproject
conda install pandas numpy scipy scikit-learn
conda env export > environment.yml

# 他人重现环境
conda env create -f environment.yml

# R: 使用renv
# 在R中运行
renv::init()  # 初始化项目
renv::snapshot()  # 保存包版本
renv::restore()  # 他人恢复环境
```

**3. 代码组织结构**
```
project/
├── data/
│   ├── raw/              # 原始数据（只读）
│   ├── processed/        # 清洗后数据
│   └── README.md         # 数据说明
├── code/
│   ├── 01_data_cleaning.py
│   ├── 02_exploratory_analysis.py
│   ├── 03_statistical_tests.py
│   └── 04_visualization.py
├── results/
│   ├── figures/
│   └── tables/
├── docs/
│   └── analysis_plan.md  # 预注册分析计划
├── environment.yml
├── README.md
└── LICENSE
```

**4. 文学编程（Literate Programming）**
```python
# 使用Jupyter Notebook或R Markdown
# 混合代码、结果、解释

"""
# 数据分析报告：虚拟细胞模型验证

## 1. 研究问题
我们假设虚拟细胞模型能预测药物响应，准确率>80%。

## 2. 数据加载
"""
import pandas as pd
df = pd.read_csv('../data/processed/cell_response.csv')
print(f"样本量: {len(df)}")

"""
## 3. 统计分析
使用逻辑回归预测响应。
"""
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
# ... 分析代码 ...

"""
## 4. 结果
模型准确率为 85.3% (95% CI: 82.1-88.5)，支持我们的假设。
"""
```

**5. 数据和代码发布**

**发布清单：**
- [ ] 代码推送到GitHub（添加清晰的README）
- [ ] 数据上传到Zenodo/Figshare（获得DOI）
- [ ] 预注册上传到OSF
- [ ] 在论文中引用所有资源的DOI

**示例README.md：**
```markdown
# 虚拟细胞药物响应预测

## 摘要
本项目包含论文"Virtual Cell Models for Precision Medicine"的所有分析代码。

## 环境要求
- Python 3.9+
- 见environment.yml

## 使用方法
1. 克隆仓库：`git clone https://github.com/username/project.git`
2. 创建环境：`conda env create -f environment.yml`
3. 运行分析：`python code/run_all.py`

## 数据
原始数据可从Zenodo下载：https://doi.org/10.5281/zenodo.xxxxx

## 引用
如使用本代码，请引用：
Author et al. (2024). Title. Journal. DOI: xxx

## 许可
MIT License
```

**工具：**
- 版本控制：Git, GitHub/GitLab
- 环境管理：conda, Docker, renv
- 文学编程：Jupyter, R Markdown, Quarto
- 数据仓库：Zenodo, Figshare, OSF
- 代码质量：pylint, black（Python格式化）

**时间投入：**
- 初始设置：4-8小时
- 日常维护：每周1-2小时
- 论文发表前整理：2-3天

---

### **学习曲线与时间投入总结**

**第1-2个月（基础期）：**
- 每天2小时学习 + 1小时实践
- 掌握数据清洗和基础统计检验
- 完成3-5个小项目（Kaggle数据集）

**第3-4个月（应用期）：**
- 每天1小时学习 + 2-3小时实战
- 在实际科研项目中应用
- 建立个人分析模板库

**第5-6个月（精通期）：**
- 每周5-10小时深度实践
- 处理复杂问题（混合效应模型、贝叶斯推断）
- 开始指导他人

**长期维持：**
- 每周3-5小时（阅读新方法、优化工作流）
- 每月参加1次统计研讨会/读书会

---

## 3. 常见陷阱与错误

### **陷阱1：p-hacking（最致命）**

**表现：**
- 尝试多种统计方法，只报告p<0.05的结果
- 收集数据后不断添加样本直到显著
- 事后选择性报告子组分析

**危害：**
- 假阳性率从5%飙升至30-60%
- 导致不可重复的研究
- 损害科学信誉，可能被撤稿

**识别方法：**
- 检查是否有预注册分析计划
- 查看是否报告了所有测试的假设
- 注意"探索性分析"是否变成"证实性结论"

**避免策略：**
```python
# 1. 预注册分析计划（在OSF上）
# 2. 使用多重比较校正
from statsmodels.stats.multitest import multipletests
reject, pvals_corrected, _, _ = multipletests(p_values, method='fdr_bh')

# 3. 报告所有测试
results = {
    'hypothesis_1': {'test': 't-test', 'p': 0.03, 'significant': True},
    'hypothesis_2': {'test': 'ANOVA', 'p': 0.12, 'significant': False},
    'hypothesis_3': {'test': 'chi-square', 'p': 0.08, 'significant': False}
}

# 4. 分离探索性和证实性分析
print("Exploratory findings (not pre-registered):")
print("Confirmatory findings (pre-registered):")
```

**补救措施：**
- 在独立数据集上验证发现
- 公开所有分析代码和决策过程
- 在论文中明确标注探索性vs证实性分析

---

### **陷阱2：忽略统计假设**

**表现：**
- 对非正态数据使用t检验
- 对异方差数据使用普通线性回归
- 对非独立数据（如重复测量）使用独立样本检验

**危害：**
- I型错误率失控（假阳性）
- 置信区间不准确
- 审稿人一眼看出问题，直接拒稿

**识别方法：**
```python
# 检验正态性
from scipy.stats import shapiro
stat, p = shapiro(data)
if p < 0.05:
    print("警告：数据非正态分布")

# 检验方差齐性
from scipy.stats import levene
stat, p = levene(group1, group2)
if p < 0.05:
    print("警告：方差不齐")

# 可视化检查
import seaborn as sns
sns.histplot(data, kde=True)  # 查看分布
sns.residplot(x=predicted, y=residuals)  # 查看残差
```

**避免策略：**
```python
# 决策树
if is_normal(data) and is_homoscedastic(data):
    # 使用参数检验
    result = stats.ttest_ind(group1, group2)
else:
    # 使用非参数检验
    result = stats.mannwhitneyu(group1, group2)
    # 或数据转换
    data_transformed = np.log(data + 1)
```

**补救措施：**
- 使用稳健性检验（bootstrap, permutation test）
- 报告敏感性分析（不同方法的结果）
- 咨询统计学家

---

### **陷阱3：过拟合（机器学习常见）**

**表现：**
- 训练集准确率99%，测试集60%
- 使用全部数据调参
- 特征数量接近或超过样本量

**危害：**
- 模型无法泛化到新数据
- 临床应用时失败
- 投资人发现后撤资

**识别方法：**
```python
from sklearn.model_selection import cross_val_score

# 交叉验证
scores = cross_val_score(model, X, y, cv=5)
print(f"CV scores: {scores}")
print(f"Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

# 学习曲线
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10))

# 如果训练和验证曲线差距大 → 过拟合
```

**避免策略：**
```python
# 1. 严格分离训练/验证/测试集
from sklearn.model_selection import train_test_split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)

# 2. 正则化
from sklearn.linear_model import LassoCV
model = LassoCV(cv=5, random_state=42)

# 3. 特征选择
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=20)
X_selected = selector.fit_transform(X_train, y_train)

# 4. 集成方法
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, max_depth=5)
```

**补救措施：**
- 收集更多数据
- 使用更简单的模型
- 在独立队列中验证

---

### **陷阱4：辛普森悖论（混淆变量）**

**表现：**
- 总体趋势与分组趋势相反
- 忽略重要协变量（如年龄、性别）

**案例：**
```
总体：治疗组死亡率30% vs 对照组20%（治疗有害？）
分层：
  轻症患者：治疗组10% vs 对照组15%（治疗有效）
  重症患者：治疗组40% vs 对照组50%（治疗有效）
原因：治疗组重症患者比例更高（选择偏倚）
```

**识别方法：**
```python
# 分层分析
for subgroup in df['severity'].unique():
    subset = df[df['severity'] == subgroup]
    print(f"{subgroup}:")
    print(subset.groupby('treatment')['death'].mean())

# 可视化
sns.catplot(data=df, x='treatment', y='death', hue='severity', kind='bar')
```

**避免策略：**
```python
# 多变量回归调整混淆
from statsmodels.formula.api import logit
model = logit('death ~ treatment + age + severity + comorbidity', data=df)
result = model.fit()
print(result.summary())

# 倾向性评分匹配
from sklearn.linear_model import LogisticRegression
ps_model = LogisticRegression()
ps_model.fit(df[['age', 'severity']], df['treatment'])
df['propensity_score'] = ps_model.predict_proba(df[['age', 'severity']])[:, 1]

# 匹配
from scipy.spatial.distance import cdist
# ... 匹配算法 ...
```

**补救措施：**
- 重新分析，加入协变量
- 进行敏感性分析
- 使用因果推断方法（DAG, do-calculus）

---

### **陷阱5：忽略多重比较**

**表现：**
- 测试100个基因，不校正p值
- 进行多个子组分析，不调整显著性水平

**危害：**
- 假阳性率爆炸（100次检验，期望5个假阳性）
- 组学研究中尤其致命

**避免策略：**
```python
from statsmodels.stats.multitest import multipletests

# FDR校正（推荐）
reject, pvals_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')

# Bonferroni（保守）
reject, pvals_corrected, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# 报告
results_df['p_value'] = p_values
results_df['p_adjusted'] = pvals_corrected
results_df['significant'] = reject
```

---

### **陷阱6：样本量不足**

**表现：**
- 事先不计算样本量
- "我们收集了所有可用数据"（但可能不够）

**危害：**
- 检验效能低，漏掉真实效应（II型错误）
- 浪费资源

**避免策略：**
```python
from statsmodels.stats.power import tt_ind_solve_power

# 计算所需样本量
n = tt_ind_solve_power(effect_size=0.5,  # Cohen's d
                       alpha=0.05,
                       power=0.8,
                       alternative='two-sided')
print(f"每组需要 {int(np.ceil(n))} 个样本")

# 或计算当前设计的检验效能
from statsmodels.stats.power import tt_ind_solve_power
power = tt_ind_solve_power(effect_size=0.5,
                           nobs1=30,  # 实际样本量
                           alpha=0.05,
                           alternative='two-sided')
print(f"检验效能: {power:.2%}")
```

---

### **陷阱7：数据泄露（机器学习）**

**表现：**
- 在分割数据前进行特征选择或标准化
- 使用未来信息预测过去

**识别：**
```python
# 错误做法
X_scaled = StandardScaler().fit_transform(X)  # 使用全部数据
X_train, X_test = train_test_split(X_scaled, y)

# 正确做法
X_train, X_test, y_train, y_test = train_test_split(X, y)
scaler = StandardScaler().fit(X_train)  # 只在训练集上拟合
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

### **陷阱8：因果推断错误**

**表现：**
- 从相关性得出因果结论
- "A与B相关，所以A导致B"

**避免：**
- 使用因果图（DAG）明确假设
- 进行随机对照试验（RCT）
- 使用工具变量、断点回归等准实验设计

---

### **陷阱9：忽略缺失数据机制**

**表现：**
- 直接删除缺失值
- 不分析缺失模式

**策略：**
```python
import missingno as msno

# 可视化缺失模式
msno.matrix(df)
msno.heatmap(df)

# 检验缺失机制
# MCAR (完全随机缺失): Little's MCAR test
# MAR (随机缺失): 逻辑回归预测缺失
# MNAR (非随机缺失): 敏感性分析
```

---

### **陷阱10：不报告置信区间和效应量**

**表现：**
- 只报告p值
- "p<0.05，显著！"

**正确做法：**
```python
from scipy import stats
import numpy as np

# t检验
t_stat, p_value = stats.ttest_ind(group1, group2)

# 计算置信区间
mean_diff = group2.mean() - group1.mean()
se = np.sqrt(group1.var()/len(group1) + group2.var()/len(group2))
ci_lower = mean_diff - 1.96 * se
ci_upper = mean_diff + 1.96 * se

# 计算效应量
cohens_d = mean_diff / np.sqrt((group1.var() + group2.var()) / 2)

# 报告
print(f"Mean difference: {mean_diff:.2f} (95% CI: {ci_lower:.2f} to {ci_upper:.2f})")
print(f"p-value: {p_value:.4f}")
print(f"Effect size (Cohen's d): {cohens_d:.2f}")
```

---

## 4. 高手策略与进阶技巧

### **策略1：贝叶斯思维（反直觉但强大）**

**与频率学派的区别：**
- 频率学派："如果零假设为真，观察到这些数据的概率是多少？"
- 贝叶斯："给定这些数据，假设为真的概率是多少？"

**实战应用：**
```python
import pymc as pm
import arviz as az

# 案例：估计药物响应率
with pm.Model() as model:
    # 先验（基于文献或专家意见）
    p = pm.Beta('p', alpha=2, beta=2)  # 响应率的先验分布
    
    # 似然
    obs = pm.Binomial('obs', n=100, p=p, observed=65)  # 100人中65人响应
    
    # 采样
    trace = pm.sample(2000, return_inferencedata=True)

# 后验分布
az.plot_posterior(trace, var_names=['p'], ref_val=0.5)
print(f"响应率后验均值: {trace.posterior['p'].mean():.3f}")
print(f"95% 可信区间: {az.hdi(trace, var_names=['p'])}")

# 决策：响应率>50%的概率
prob_better = (trace.posterior['p'] > 0.5).mean()
print(f"P(响应率>50% | 数据) = {prob_better:.2%}")
```

**优势：**
- 直接回答"假设为真的概率"
- 自然融合先验知识
- 小样本时更稳健
- 提供完整的不确定性量化

**何时使用：**
- 样本量小（<30）
- 有强先验信息（如药物同类研究）
- 需要序贯决策（临床试验中期分析）
- 层次模型（多中心研究）

---

### **策略2：因果推断（从相关到因果）**

**工具：有向无环图（DAG）**
```python
import dagitty
from causalgraphicalmodels import CausalGraphicalModel

# 定义因果图
causal_graph = CausalGraphicalModel(
    nodes=["Treatment", "Outcome", "Age", "Severity"],
    edges=[
        ("Treatment", "Outcome"),
        ("Age", "Treatment"),
        ("Age", "Outcome"),
        ("Severity", "Treatment"),
        ("Severity", "Outcome")
    ]
)

# 识别需要调整的变量
adjustment_set = causal_graph.get_adjustment_sets("Treatment", "Outcome")
print(f"需要调整的变量: {adjustment_set}")

# 使用双重差分（DID）
# 适用于政策评估、自然实验
import statsmodels.formula.api as smf

model = smf.ols('outcome ~ treatment * post + age + severity', data=df)
result = model.fit()
print(f"因果效应: {result.params['treatment:post']}")
```

**工具变量（IV）：**
```python
from linearmodels.iv import IV2SLS

# 案例：估计教育对收入的因果效应
# 工具变量：出生季度（影响教育但不直接影响收入）
model = IV2SLS.from_formula('income ~ 1 + [education ~ birth_quarter] + age', data=df)
result = model.fit()
print(result.summary)
```

**断点回归（RDD）：**
```python
# 案例：某政策在年龄65岁开始实施
import numpy as np
from scipy.optimize import curve_fit

# 拟合断点两侧的回归
below_65 = df[df['age'] < 65]
above_65 = df[df['age'] >= 65]

# 计算断点处的跳跃（因果效应）
# ... RDD分析代码 ...
```

---

### **策略3：机器学习与统计推断的融合**

**问题：**机器学习擅长预测，但难以推断（黑箱）

**解决方案：可解释AI + 统计推断**

```python
# 1. SHAP值（解释任何模型）
import shap
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
model.fit(X_train, y_train)

# 计算SHAP值
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# 可视化
shap.summary_plot(shap_values, X_test)
shap.dependence_plot("gene_expression", shap_values, X_test)

# 2. 排列重要性（模型无关）
from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(model, X_test, y_test, n_repeats=30)
for i, (mean, std) in enumerate(zip(perm_importance.importances_mean, 
                                     perm_importance.importances_std)):
    print(f"{feature_names[i]}: {mean:.3f} +/- {std:.3f}")

# 3. 部分依赖图（PDP）
from sklearn.inspection import PartialDependenceDisplay

PartialDependenceDisplay.from_estimator(model, X_train, features=[0, 1, (0, 1)])
```

**双重机器学习（DML）：**
```python
from econml.dml import LinearDML

# 估计治疗效应，同时控制高维协变量
dml = LinearDML(model_y=RandomForestRegressor(),
                model_t=RandomForestClassifier())
dml.fit(Y=y, T=treatment, X=X_confounders)

# 治疗效应及置信区间
effect = dml.effect(X_test)
ci = dml.effect_interval(X_test, alpha=0.05)
```

---

### **策略4：稳健性检验（让审稿人无话可说）**

**多角度验证结果：**

```python
# 1. 不同统计方法
results = {}
results['t-test'] = stats.ttest_ind(group1, group2).pvalue
results['mann-whitney'] = stats.mannwhitneyu(group1, group2).pvalue
results['permutation'] = permutation_test(group1, group2)
results['bootstrap'] = bootstrap_test(group1, group2)

print("Robustness check:")
for method, p in results.items():
    print(f"{method}: p = {p:.4f}")

# 2. 子集分析
for subset in ['male', 'female', 'young', 'old']:
    subset_data = df[df['group'] == subset]
    # 重复分析
    
# 3. 敏感性分析（改变假设）
for threshold in [0.1, 0.2, 0.3]:
    # 改变分类阈值
    # 重复分析

# 4. 留一法交叉验证
from sklearn.model_selection import LeaveOneOut
loo = LeaveOneOut()
scores = []
for train_idx, test_idx in loo.split(X):
    # 训练和评估
    scores.append(score)
print(f"LOO CV score: {np.mean(scores):.3f}")
```

---

### **策略5：元分析（整合多个研究）**

**当你需要：**
- 综合文献中的证据
- 提高统计效能
- 发表高影响力综述

```python
import numpy as np
from scipy.stats import norm

# 固定效应元分析
def fixed_effects_meta_analysis(effect_sizes, variances):
    weights = 1 / np.array(variances)
    pooled_effect = np.sum(weights * effect_sizes) / np.sum(weights)
    pooled_se = np.sqrt(1 / np.sum(weights))
    
    z = pooled_effect / pooled_se
    p_value = 2 * (1 - norm.cdf(abs(z)))
    
    ci_lower = pooled_effect - 1.96 * pooled_se
    ci_upper = pooled_effect + 1.96 * pooled_se
    
    return {
        'pooled_effect': pooled_effect,
        'ci': (ci_lower, ci_upper),
        'p_value': p_value
    }

# 示例数据（从5个研究中提取）
effect_sizes = [0.3, 0.5, 0.4, 0.6, 0.35]
variances = [0.01, 0.02, 0.015, 0.025, 0.012]

result = fixed_effects_meta_analysis(effect_sizes, variances)
print(f"Pooled effect: {result['pooled_effect']:.3f}")
print(f"95% CI: {result['ci']}")
print(f"p-value: {result['p_value']:.4f}")

# 异质性检验（I²统计量）
# 森林图可视化
# ...
```

---

### **策略6：自适应设计（临床试验高级技巧）**

**传统vs自适应：**
- 传统：固定样本量，试验结束后分析
- 自适应：中期分析，可调整样本量或停止试验

```python
# 序贯分析（Group Sequential Design）
from scipy.stats import norm

def obrien_fleming_boundary(n_looks, alpha=0.05):
    """O'Brien-Fleming边界（保守）"""
    z_alpha = norm.ppf(1 - alpha/2)
    boundaries = [z_alpha * np.sqrt(n_looks / i) for i in range(1, n_looks+1)]
    return boundaries

# 3次中期分析
boundaries = obrien_fleming_boundary(n_looks=3, alpha=0.05)
print(f"停止边界（Z值）: {boundaries}")

# 在每次中期分析时
z_current = (effect_observed - 0) / se_observed
if abs(z_current) > boundaries[current_look]:
    print("达到显著性，可提前停止试验")
```

---

### **策略7：高维数据的降维与特征工程**

**组学数据的挑战：**
- 特征数（基因）>> 样本数
- 多重共线性
- 噪音大

**解决方案：**

```python
# 1. 方差过滤
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.1)
X_filtered = selector.fit_transform(X)

# 2. 单变量特征选择
from sklearn.feature_selection import SelectKBest, f_classif
selector = SelectKBest(f_classif, k=100)
X_selected = selector.fit_transform(X, y)

# 3. LASSO（自动特征选择）
from sklearn.linear_model import LassoCV
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X, y)
selected_features = np.where(lasso.coef_ != 0)[0]

# 4. 主成分分析（PCA）
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)  # 保留95%方差
X_pca = pca.fit_transform(X)

# 5. 弹性网络（LASSO + Ridge）
from sklearn.linear_model import ElasticNetCV
enet = ElasticNetCV(cv=5, l1_ratio=[.1, .5, .7, .9, .95, .99, 1])
enet.fit(X, y)
```

---

### **策略8：处理不平衡数据**

**问题：**疾病样本100例，健康对照10000例

**策略：**
```python
# 1. 重采样
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# 组合过采样和欠采样
over = SMOTE(sampling_strategy=0.5)
under = RandomUnderSampler(sampling_strategy=0.8)
pipeline = Pipeline([('over', over), ('under', under)])
X_resampled, y_resampled = pipeline.fit_resample(X, y)

# 2. 类权重
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(class_weight='balanced')

# 3. 评估指标选择
from sklearn.metrics import (precision_recall_curve, average_precision_score,
                             roc_auc_score, f1_score)

# 不要只看准确率！
print(f"AUROC: {roc_auc_score(y_true, y_pred_proba):.3f}")
print(f"AUPRC: {average_precision_score(y_true, y_pred_proba):.3f}")
print(f"F1-score: {f1_score(y_true, y_pred):.3f}")
```

---

### **策略9：时间序列与纵向数据**

**混合效应模型（处理重复测量）：**
```python
import statsmodels.formula.api as smf

# 每个患者多次测量
model = smf.mixedlm("biomarker ~ time + treatment + time:treatment",
                    data=df,
                    groups=df["patient_id"])
result = model.fit()
print(result.summary())

# 随机截距和斜率
model = smf.mixedlm("biomarker ~ time + treatment",
                    data=df,
                    groups=df["patient_id"],
                    re_formula="~time")
```

**时间序列预测：**
```python
from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet

# ARIMA
model = ARIMA(time_series, order=(1, 1, 1))
result = model.fit()
forecast = result.forecast(steps=30)

# Prophet（Facebook开发，处理季节性）
df_prophet = pd.DataFrame({'ds': dates, 'y': values})
model = Prophet()
model.fit(df_prophet)
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)
```

---

### **策略10：贝叶斯优化（超参数调优）**

**比网格搜索快10-100倍：**
```python
from skopt import BayesSearchCV
from sklearn.ensemble import RandomForestClassifier

# 定义搜索空间
param_space = {
    'n_estimators': (10, 200),
    'max_depth': (3, 20),
    'min_samples_split': (2, 20),
    'min_samples_leaf': (1, 10)
}

# 贝叶斯优化
opt = BayesSearchCV(
    RandomForestClassifier(),
    param_space,
    n_iter=50,
    cv=5,
    n_jobs=-1,
    random_state=42
)

opt.fit(X_train, y_train)
print(f"Best params: {opt.best_params_}")
print(f"Best score: {opt.best_score_:.3f}")
```

---

## 5. 评估与迭代

### **水平自评体系**

**初级（0-3个月）：**
- [ ] 能独立完成数据清洗（缺失值、异常值处理）
- [ ] 掌握5种基础统计检验（t检验、卡方、相关、ANOVA、回归）
- [ ] 能制作出版级图表（ggplot2或seaborn）
- [ ] 理解p值、置信区间、效应量的含义
- [ ] 能用Git管理分析代码

**中级（3-6个月）：**
- [ ] 能正确选择统计方法（根据数据类型和研究设计）
- [ ] 掌握多重比较校正、混合效应模型
- [ ] 能处理高维数据（降维、特征选择）
- [ ] 能进行机器学习建模和交叉验证
- [ ] 能写出可重复的分析报告（R Markdown/Jupyter）
- [ ] 能识别常见统计陷阱（p-hacking、过拟合等）

**高级（6-12个月）：**
- [ ] 能进行因果推断（DAG、工具变量、DID）
- [ ] 掌握贝叶斯统计（PyMC/Stan）
- [ ] 能设计临床试验（样本量计算、自适应设计）
- [ ] 能进行元分析和系统综述
- [ ] 能指导他人进行统计分析
- [ ] 发表过使用高级统计方法的论文

**专家级（1年+）：**
- [ ] 能开发新的统计方法或改进现有方法
- [ ] 能处理极端复杂的数据（多组学整合、空间转录组）
- [ ] 能担任统计审稿人
- [ ] 在统计方法学期刊发表论文
- [ ] 能用统计解决跨学科问题

---

### **具体评估指标**

**技能测试：**
```python
# 每月自测题（30分钟限时）

# 1. 数据清洗（10分钟）
# 给定一个杂乱数据集，清洗并报告：
# - 缺失值比例
# - 异常值数量
# - 数据类型错误

# 2. 统计检验（10分钟）
# 给定研究问题，选择合适方法并解释：
# - 比较3组患者的生存时间
# - 预测疾病风险（10个预测因子）
# - 分析重复测量数据

# 3. 结果解释（10分钟）
# 给定分析输出，写一段结果描述：
# - 包含效应量、置信区间、p值
# - 用通俗语言解释
# - 指出潜在问题
```

**项目评估：**
- **速度：**从原始数据到可发表图表的时间
  - 初级：2-3天
  - 中级：1天
  - 高级：4-6小时
  
- **质量：**代码审查清单
  - [ ] 代码可运行（他人能重现）
  - [ ] 有清晰注释
  - [ ] 统计方法正确
  - [ ] 图表符合期刊要求
  - [ ] 有敏感性分析

- **影响力：**
  - 论文引用统计分析的次数
  - 合作者主动寻求统计帮助的频率
  - 在组会上能解答统计问题

---

### **反馈获取渠道**

**1. 同行评审：**
- 加入统计学习小组（每周讨论一篇方法学论文）
- 在Cross Validated (stats.stackexchange.com) 提问和回答
- 参加统计研讨会

**2. 导师反馈：**
- 找一位统计学家mentor（每月1次咨询）
- 在提交论文前请统计学家审查方法部分
- 参加统计咨询门诊（很多大学提供）

**3. 实战反馈：**
- Kaggle竞赛（排名是客观指标）
- 审稿人意见（论文被拒后的统计批评）
- 数据分析咨询项目（客户满意度）

**4. 自动化反馈：**
```python
# 代码质量检查
# 使用pylint
!pylint my_analysis.py

# 统计方法检查
# 自己写一个checklist函数
def statistical_checklist(data, method):
    checks = {
        'sample_size': len(data) >= 30,
        'normality_tested': True,  # 手动确认
        'effect_size_reported': True,
        'ci_reported': True,
        'multiple_testing_corrected': True
    }
    return checks
```

---

### **持续改进机制**

**每周：**
- 阅读1篇统计方法学论文（周五下午）
- 在实际项目中应用1个新方法
- 记录遇到的问题和解决方案（统计日记）

**每月：**
- 完成1个Kaggle竞赛或练习项目
- 参加1次统计研讨会/webinar
- 更新个人统计工具库（代码模板）

**每季度：**
- 深度学习1个高级主题（如贝叶斯、因果推断）
- 指导1位同事进行统计分析
- 写1篇方法学博客或教程

**每年：**
- 参加1次统计学会议（如JSM, ENAR）
- 发表1篇使用高级统计的论文
- 贡献1个开源统计工具

---

## 6. 资源与工具推荐

### **必读书籍（按优先级）**

**1. 《Statistical Rethinking》 - Richard McElreath**
- **为什么：**最佳贝叶斯统计入门，用因果图思考
- **适合：**有基础统计背景，想升级思维方式
- **时间投入：**3个月（配合视频课程）
- **实践：**每章都有R/Python代码

**2. 《The Elements of Statistical Learning》 

---

**文档编号**: 058  
**生成模型**: Claude-Sonnet-4.5  
**文档类型**: 实操指南
